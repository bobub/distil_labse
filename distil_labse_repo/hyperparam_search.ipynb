{
 "cells": [
  {
   "cell_type": "raw",
   "id": "df244d53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4ab527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill\n",
    "from utils import *\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from torch.optim import Adam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e537763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(48,80),(96,40),(24,160),(12,320)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f340adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = len(pairs) # number of parameter combinations to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2108b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER CANDIDATES - try out whatever tickles your fancy\n",
    "\n",
    "possible_params = {\n",
    "    # general training parameters\n",
    "    'LEARNING_RATE':[1e-4, 3e-4, 5e-4, 6e-4, 7e-4],\n",
    "    #'SCHEDULER_CLASS':[get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_constant_schedule_with_warmup],\n",
    "    'TEMPERATURE':[9],\n",
    "    #'OPTIMIZER':['AdamW', 'AdamW(weight_decay=0.01)', 'Adam(weight_decay=0.01)'],\n",
    "    'BATCH_SIZE':[256],\n",
    "    # switch transformer - student config\n",
    "    'D_MODEL':[768],\n",
    "    'HEADS':[12],\n",
    "    'D_FF':[768, 384, 192, 96],\n",
    "    'N_LAYERS':[3],\n",
    "    'N_EXPERTS':[16,24,32,40,48,56,64,72,80,88]  \n",
    "}\n",
    "\n",
    "params = generate_random_params(possible_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e96ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['LEARNING_RATE'] = 5e-4\n",
    "params['N_LAYERS'] = 2\n",
    "params['NUM_STEPS'] = 10000\n",
    "params['TRAINING_DATA'] = 10000000\n",
    "params['BATCH_SIZE'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2def71f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1's params: {'LEARNING_RATE': 0.0005, 'TEMPERATURE': 9, 'BATCH_SIZE': 128, 'D_MODEL': 768, 'HEADS': 12, 'D_FF': 48, 'N_LAYERS': 2, 'N_EXPERTS': 80, 'NUM_STEPS': 10000, 'TRAINING_DATA': 10000000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1b641ec29f41cc8191ee45e64c9820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/47 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PapermillExecutionError",
     "evalue": "\n---------------------------------------------------------------------------\nException encountered at \"In [26]\":\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n<ipython-input-26-cde54dc8f34f> in <module>\n     20 with distiller:\n     21     distiller.train(optimizer,dataloader, num_steps=NUM_STEPS, \n---> 22     scheduler_class=SCHEDULER_CLASS, scheduler_args = scheduler_args, callback=callback_fun)\n     23 end = time.time()\n     24 \n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_basic.py in train(self, optimizer, dataloader, num_epochs, scheduler_class, scheduler_args, scheduler, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n    298         assert not (num_epochs is None and num_steps is None)\n    299         if num_steps is not None:\n--> 300             self.train_with_num_steps(optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n    301         else:\n    302             self.train_with_num_epochs(optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_epochs, callback, batch_postprocessor, **args)\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_basic.py in train_with_num_steps(self, optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n    179                 print('Global Step: ',global_step,' of ',total_global_steps)\n    180                 if global_step%(0.1*num_steps)==0:\n--> 181                     valid_loss = self.save_and_callback(global_step, step, 0, callback, optimizer, losses_dict, total_loss)\n    182                     self.write_valid_loss(valid_loss.item(), writer_step-1)\n    183                 if self.d_config.kd_loss_weight_scheduler is not None:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_general.py in save_and_callback(self, global_step, step, epoch, callback, optimizer, losses_dict, total_loss)\n     73             self.model_T._forward_hooks = OrderedDict()\n     74 \n---> 75         valid_loss = super(GeneralDistiller, self).save_and_callback(global_step, step, epoch, callback, optimizer, losses_dict, total_loss)\n     76 \n     77         if self.has_custom_matches:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_basic.py in save_and_callback(self, global_step, step, epoch, callback, optimizer, losses_dict, total_loss)\n     37                     'total_loss':total_loss\n     38             }\n---> 39             torch.save(dict, os.path.join(self.t_config.output_dir, f\"model_{global_step}.pkl\"))\n     40             if self.local_rank == 0:\n     41                 torch.distributed.barrier()\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\n    367     _check_dill_version(pickle_module)\n    368 \n--> 369     with _open_file_like(f, 'wb') as opened_file:\n    370         if _use_new_zipfile_serialization:\n    371             with _open_zipfile_writer(opened_file) as opened_zipfile:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py in _open_file_like(name_or_buffer, mode)\n    228 def _open_file_like(name_or_buffer, mode):\n    229     if _is_path(name_or_buffer):\n--> 230         return _open_file(name_or_buffer, mode)\n    231     else:\n    232         if 'w' in mode:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py in __init__(self, name, mode)\n    209 class _open_file(_opener):\n    210     def __init__(self, name, mode):\n--> 211         super(_open_file, self).__init__(open(name, mode))\n    212 \n    213     def __exit__(self, *args):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'models/switch/time-20210606-182247/model_1000.pkl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPapermillExecutionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c6474035451b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m'outputs/distillation_{}.ipynb'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mkernel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/papermill/execute.py\u001b[0m in \u001b[0;36mexecute_notebook\u001b[0;34m(input_path, output_path, parameters, engine_name, request_save_on_cell_execute, prepare_only, kernel_name, language, progress_bar, log_output, stdout_file, stderr_file, start_timeout, report_mode, cwd, **engine_kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Check for errors first (it saves on error before raising)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mraise_for_execution_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Write final output in case the engine didn't write it on cell completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/papermill/execute.py\u001b[0m in \u001b[0;36mraise_for_execution_errors\u001b[0;34m(nb, output_path)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mwrite_ipynb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPapermillExecutionError\u001b[0m: \n---------------------------------------------------------------------------\nException encountered at \"In [26]\":\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n<ipython-input-26-cde54dc8f34f> in <module>\n     20 with distiller:\n     21     distiller.train(optimizer,dataloader, num_steps=NUM_STEPS, \n---> 22     scheduler_class=SCHEDULER_CLASS, scheduler_args = scheduler_args, callback=callback_fun)\n     23 end = time.time()\n     24 \n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_basic.py in train(self, optimizer, dataloader, num_epochs, scheduler_class, scheduler_args, scheduler, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n    298         assert not (num_epochs is None and num_steps is None)\n    299         if num_steps is not None:\n--> 300             self.train_with_num_steps(optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n    301         else:\n    302             self.train_with_num_epochs(optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_epochs, callback, batch_postprocessor, **args)\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_basic.py in train_with_num_steps(self, optimizer, scheduler, tqdm_disable, dataloader, max_grad_norm, num_steps, callback, batch_postprocessor, **args)\n    179                 print('Global Step: ',global_step,' of ',total_global_steps)\n    180                 if global_step%(0.1*num_steps)==0:\n--> 181                     valid_loss = self.save_and_callback(global_step, step, 0, callback, optimizer, losses_dict, total_loss)\n    182                     self.write_valid_loss(valid_loss.item(), writer_step-1)\n    183                 if self.d_config.kd_loss_weight_scheduler is not None:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_general.py in save_and_callback(self, global_step, step, epoch, callback, optimizer, losses_dict, total_loss)\n     73             self.model_T._forward_hooks = OrderedDict()\n     74 \n---> 75         valid_loss = super(GeneralDistiller, self).save_and_callback(global_step, step, epoch, callback, optimizer, losses_dict, total_loss)\n     76 \n     77         if self.has_custom_matches:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/textbrewer-0.2.1.post1-py3.6.egg/textbrewer/distiller_basic.py in save_and_callback(self, global_step, step, epoch, callback, optimizer, losses_dict, total_loss)\n     37                     'total_loss':total_loss\n     38             }\n---> 39             torch.save(dict, os.path.join(self.t_config.output_dir, f\"model_{global_step}.pkl\"))\n     40             if self.local_rank == 0:\n     41                 torch.distributed.barrier()\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\n    367     _check_dill_version(pickle_module)\n    368 \n--> 369     with _open_file_like(f, 'wb') as opened_file:\n    370         if _use_new_zipfile_serialization:\n    371             with _open_zipfile_writer(opened_file) as opened_zipfile:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py in _open_file_like(name_or_buffer, mode)\n    228 def _open_file_like(name_or_buffer, mode):\n    229     if _is_path(name_or_buffer):\n--> 230         return _open_file(name_or_buffer, mode)\n    231     else:\n    232         if 'w' in mode:\n\n~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py in __init__(self, name, mode)\n    209 class _open_file(_opener):\n    210     def __init__(self, name, mode):\n--> 211         super(_open_file, self).__init__(open(name, mode))\n    212 \n    213     def __exit__(self, *args):\n\nFileNotFoundError: [Errno 2] No such file or directory: 'models/switch/time-20210606-182247/model_1000.pkl'\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(iterations):\n",
    "    params['D_FF'] = pairs[i][0]\n",
    "    params['N_EXPERTS'] = pairs[i][1]\n",
    "    print('Iteration {}\\'s params: {}\\n'.format(i+1,params))\n",
    "    papermill.execute_notebook(\n",
    "    'distillation.ipynb',\n",
    "    'outputs/distillation_{}.ipynb'.format(i+1),\n",
    "    kernel_name='python3',\n",
    "    parameters=params\n",
    "    )\n",
    "    end = time.time()\n",
    "    print('Time (s) passed since last iteration: ',end-start)\n",
    "    start = end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf51141",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9363b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/anaconda3/envs/pytorch_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d80d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660dceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
